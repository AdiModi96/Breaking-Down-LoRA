{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from models import MultiLayeredPerceptron as mlp\n",
    "from lora_models import LoRAModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LoRAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size=(4, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayeredPerceptron(\n",
      "  (linear_0): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (dropout_0): Dropout(p=0.2, inplace=False)\n",
      "  (relu_0): ReLU(inplace=True)\n",
      "  (linear_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "  (relu_1): ReLU(inplace=True)\n",
      "  (linear_2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "  (relu_2): ReLU(inplace=True)\n",
      "  (output): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Num trainable/non-trainable parameters in model: 932362/0\n"
     ]
    }
   ],
   "source": [
    "model = mlp()\n",
    "print(model)\n",
    "\n",
    "num_trainable_parameters_in_model = 0\n",
    "num_non_trainable_parameters_in_model = 0\n",
    "for parameter in model.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        num_trainable_parameters_in_model += parameter.numel()\n",
    "    else:\n",
    "        num_non_trainable_parameters_in_model += parameter.numel()\n",
    "print(f'Num trainable/non-trainable parameters in model: {num_trainable_parameters_in_model}/{num_non_trainable_parameters_in_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRAModel(\n",
      "  (base_model): MultiLayeredPerceptron(\n",
      "    (linear_0): LoRALinear(Linear(in_features=784, out_features=512, bias=True) + ((α=2/r=4) × Adapter(in_features=784, rank=4, out_features=512, delta_bias=False)))\n",
      "    (dropout_0): Dropout(p=0.2, inplace=False)\n",
      "    (relu_0): ReLU(inplace=True)\n",
      "    (linear_1): LoRALinear(Linear(in_features=512, out_features=512, bias=True) + ((α=2/r=4) × Adapter(in_features=512, rank=4, out_features=512, delta_bias=False)))\n",
      "    (dropout_1): Dropout(p=0.2, inplace=False)\n",
      "    (relu_1): ReLU(inplace=True)\n",
      "    (linear_2): LoRALinear(Linear(in_features=512, out_features=512, bias=True) + ((α=2/r=4) × Adapter(in_features=512, rank=4, out_features=512, delta_bias=False)))\n",
      "    (dropout_2): Dropout(p=0.2, inplace=False)\n",
      "    (relu_2): ReLU(inplace=True)\n",
      "    (output): LoRALinear(Linear(in_features=512, out_features=10, bias=True) + ((α=2/r=4) × Adapter(in_features=512, rank=4, out_features=10, delta_bias=False)))\n",
      "  )\n",
      ")\n",
      "Num trainable/non-trainable parameters in LoRA Model: 15464/932370\n"
     ]
    }
   ],
   "source": [
    "lora_model = LoRAModel()\n",
    "lora_model.add_base_model(base_model=model)\n",
    "lora_config = {\n",
    "    'rank': 4,\n",
    "    'alpha': 2,\n",
    "    'delta_bias': False\n",
    "}\n",
    "lora_target_module_names = ['linear_*', 'output']\n",
    "lora_model.build_new_adapter(lora_target_module_names=lora_target_module_names, lora_config=lora_config)\n",
    "print(lora_model)\n",
    "\n",
    "num_trainable_parameters_in_lora_model = 0\n",
    "num_non_trainable_parameters_in_lora_model = 0\n",
    "for parameter in lora_model.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        num_trainable_parameters_in_lora_model += parameter.numel()\n",
    "    else:\n",
    "        num_non_trainable_parameters_in_lora_model += parameter.numel()\n",
    "print(f'Num trainable/non-trainable parameters in LoRA Model: {num_trainable_parameters_in_lora_model}/{num_non_trainable_parameters_in_lora_model}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0750,  0.0100,  0.0068,  0.0678, -0.0509,  0.0307,  0.0163,  0.0675,\n",
       "         -0.0730, -0.0218],\n",
       "        [-0.0088, -0.0137,  0.0012,  0.0447, -0.0641,  0.0054, -0.0130,  0.0240,\n",
       "         -0.0884,  0.0288],\n",
       "        [-0.0220,  0.0503, -0.0186,  0.1189, -0.0717,  0.0893,  0.0617,  0.0566,\n",
       "         -0.0444,  0.0046],\n",
       "        [-0.0362,  0.0226, -0.0505,  0.0802, -0.0975,  0.0138, -0.0094,  0.0921,\n",
       "         -0.0454,  0.0067]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "# model.linear_0(x)\n",
    "# model.linear_1(model.linear_0(x))\n",
    "# model.linear_2(model.linear_1(model.linear_0(x)))\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0750,  0.0100,  0.0068,  0.0678, -0.0509,  0.0307,  0.0163,  0.0675,\n",
       "         -0.0730, -0.0218],\n",
       "        [-0.0088, -0.0137,  0.0012,  0.0447, -0.0641,  0.0054, -0.0130,  0.0240,\n",
       "         -0.0884,  0.0288],\n",
       "        [-0.0220,  0.0503, -0.0186,  0.1189, -0.0717,  0.0893,  0.0617,  0.0566,\n",
       "         -0.0444,  0.0046],\n",
       "        [-0.0362,  0.0226, -0.0505,  0.0802, -0.0975,  0.0138, -0.0094,  0.0921,\n",
       "         -0.0454,  0.0067]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.disable_adapter()\n",
    "lora_model.eval()\n",
    "# lora_model.base_model.linear_0(x)\n",
    "# lora_model.base_model.linear_1(lora_model.base_model.linear_0(x))\n",
    "# lora_model.base_model.linear_1(lora_model.base_model.linear_1(lora_model.base_model.linear_0(x)))\n",
    "lora_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-217355.9375, -293397.1875,   38750.7031,  140768.0156,  -61815.7070,\n",
       "          -49068.4023,  135163.4062,   23110.6602,  216566.7344, -532693.8750],\n",
       "        [ -41830.3438,  -56958.0977,   18479.0781,    2725.8491,   21184.8359,\n",
       "            9369.0840,   22974.5195,   12114.3467,   32646.7109,  -37039.6445],\n",
       "        [-202302.9531, -282877.9375,   43790.0547,   22540.9668,   68376.0859,\n",
       "           27912.7383,  126027.2109,   64679.0977,  176971.3125, -226896.2500],\n",
       "        [-132472.2344, -170983.7812,   20667.1797,   55350.4727,   14791.7998,\n",
       "           -1081.7465,   81299.0078,   26734.1914,  118614.3828, -230755.5156]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.enable_adapter()\n",
    "lora_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-217355.9375, -293397.4062,   38750.6953,  140768.0156,  -61815.7188,\n",
       "          -49068.4375,  135163.4219,   23110.6562,  216566.7188, -532693.8750],\n",
       "        [ -41830.3477,  -56958.1094,   18479.0703,    2725.8516,   21184.8555,\n",
       "            9369.0898,   22974.5176,   12114.3535,   32646.7031,  -37039.6641],\n",
       "        [-202302.9375, -282877.9062,   43790.0547,   22540.8750,   68376.2031,\n",
       "           27912.7734,  126027.1953,   64679.1836,  176971.3281, -226896.0312],\n",
       "        [-132472.1562, -170983.8281,   20667.1250,   55350.4297,   14791.7969,\n",
       "           -1081.7734,   81299.0312,   26734.2031,  118614.3906, -230755.4062]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = lora_model.get_merged_model()\n",
    "merged_model.eval()\n",
    "merged_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_0', 'linear_1', 'linear_2', 'output']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.lora_module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
