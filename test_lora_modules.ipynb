{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbcbb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from lora_modules import LoRALinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e713db0",
   "metadata": {},
   "source": [
    "# Testing LoRALinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc04209",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(size=(4, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c392a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = 16\n",
    "out_features = 32\n",
    "bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e2315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=16, out_features=32, bias=True)\n",
      "Num trainable/non-trainable parameters in Linear: 544/0\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(in_features=in_features, out_features=out_features, bias=bias)\n",
    "print(linear)\n",
    "\n",
    "num_trainable_parameters_in_linear = 0\n",
    "num_non_trainable_parameters_in_linear = 0\n",
    "for parameter in linear.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        num_trainable_parameters_in_linear += parameter.numel()\n",
    "    else:\n",
    "        num_non_trainable_parameters_in_linear += parameter.numel()\n",
    "print(f'Num trainable/non-trainable parameters in Linear: {num_trainable_parameters_in_linear}/{num_non_trainable_parameters_in_linear}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8363301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3112, -0.5434,  0.5286, -0.3006, -0.2723, -0.3482, -0.1714, -0.2795,\n",
       "         -0.8456,  0.3614,  0.3751, -0.4710, -0.4060, -0.4528,  0.1198, -0.3354,\n",
       "         -0.0631, -0.3033,  0.2364,  0.2626, -0.4088, -0.4054, -0.3306,  0.0158,\n",
       "         -0.2222,  0.3794, -0.4798, -0.3367, -0.1746, -0.5843,  0.0875,  0.4903],\n",
       "        [-0.2201, -0.4519,  0.5687,  0.7261, -1.4542, -0.3455, -0.5674, -0.5083,\n",
       "          0.5267,  0.2598, -0.2588, -0.1397,  0.1017,  0.8528,  0.0521,  0.9468,\n",
       "         -0.0486, -0.1825, -0.4143, -0.6748, -0.2408,  0.4069,  0.4898,  0.9341,\n",
       "         -0.2967,  0.3735, -0.4919, -0.0398,  1.0945,  0.5666, -0.0376,  0.5202],\n",
       "        [-0.5604,  0.6451,  0.1797, -0.6332,  0.6717,  0.2009,  1.1105, -1.0070,\n",
       "          0.3122, -0.3347,  0.2356,  0.0648,  0.9382, -0.4550,  0.6496, -0.6390,\n",
       "         -0.1045, -0.5934,  0.7739, -0.2603,  0.4633,  0.9035,  0.7268, -0.6977,\n",
       "         -0.5872, -0.4794,  0.0224, -0.4526,  0.2953,  0.4776,  0.3914, -0.1528],\n",
       "        [-0.3303, -0.8498,  0.5489,  1.1919,  0.7467, -0.3974,  0.9824, -0.1678,\n",
       "          0.1932,  0.6756,  0.6278, -0.0841,  0.8882, -0.8669, -0.0345, -0.6671,\n",
       "          0.5580, -0.5825,  0.7146, -0.9386, -0.1250,  0.4492, -0.2384,  0.1074,\n",
       "         -0.2847, -0.1802, -0.2668,  0.1220,  1.0887,  0.4816,  0.8641,  0.3498]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d58ba81-ea6c-44d0-a6a0-6fc4686ab6d2",
   "metadata": {},
   "source": [
    "linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f39ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = {\n",
    "    'rank': 4,\n",
    "    'alpha': 2,\n",
    "    'delta_bias': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17c113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRALinear(Linear(in_features=16, out_features=32, bias=True) + ((α=2/r=4) × Adapter(in_features=16, rank=4, out_features=32, delta_bias=False)))\n",
      "Num trainable/non-trainable parameters in LoRA Linear: 736/2\n"
     ]
    }
   ],
   "source": [
    "lora_linear = LoRALinear()\n",
    "lora_linear.add_base_module(base_module=linear)\n",
    "lora_linear.build_new_adapter(lora_config=lora_config)\n",
    "print(lora_linear)\n",
    "\n",
    "num_trainable_parameters_in_lora_linear = 0\n",
    "num_non_trainable_parameters_in_lora_linear = 0\n",
    "for parameter in lora_linear.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        num_trainable_parameters_in_lora_linear += parameter.numel()\n",
    "    else:\n",
    "        num_non_trainable_parameters_in_lora_linear += parameter.numel()\n",
    "print(f'Num trainable/non-trainable parameters in LoRA Linear: {num_trainable_parameters_in_lora_linear}/{num_non_trainable_parameters_in_lora_linear}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18572f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3112, -0.5434,  0.5286, -0.3006, -0.2723, -0.3482, -0.1714, -0.2795,\n",
       "         -0.8456,  0.3614,  0.3751, -0.4710, -0.4060, -0.4528,  0.1198, -0.3354,\n",
       "         -0.0631, -0.3033,  0.2364,  0.2626, -0.4088, -0.4054, -0.3306,  0.0158,\n",
       "         -0.2222,  0.3794, -0.4798, -0.3367, -0.1746, -0.5843,  0.0875,  0.4903],\n",
       "        [-0.2201, -0.4519,  0.5687,  0.7261, -1.4542, -0.3455, -0.5674, -0.5083,\n",
       "          0.5267,  0.2598, -0.2588, -0.1397,  0.1017,  0.8528,  0.0521,  0.9468,\n",
       "         -0.0486, -0.1825, -0.4143, -0.6748, -0.2408,  0.4069,  0.4898,  0.9341,\n",
       "         -0.2967,  0.3735, -0.4919, -0.0398,  1.0945,  0.5666, -0.0376,  0.5202],\n",
       "        [-0.5604,  0.6451,  0.1797, -0.6332,  0.6717,  0.2009,  1.1105, -1.0070,\n",
       "          0.3122, -0.3347,  0.2356,  0.0648,  0.9382, -0.4550,  0.6496, -0.6390,\n",
       "         -0.1045, -0.5934,  0.7739, -0.2603,  0.4633,  0.9035,  0.7268, -0.6977,\n",
       "         -0.5872, -0.4794,  0.0224, -0.4526,  0.2953,  0.4776,  0.3914, -0.1528],\n",
       "        [-0.3303, -0.8498,  0.5489,  1.1919,  0.7467, -0.3974,  0.9824, -0.1678,\n",
       "          0.1932,  0.6756,  0.6278, -0.0841,  0.8882, -0.8669, -0.0345, -0.6671,\n",
       "          0.5580, -0.5825,  0.7146, -0.9386, -0.1250,  0.4492, -0.2384,  0.1074,\n",
       "         -0.2847, -0.1802, -0.2668,  0.1220,  1.0887,  0.4816,  0.8641,  0.3498]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_linear.disable_adapter()\n",
    "lora_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db18a883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.3009,  -2.4167,   3.1948,  -5.2583,  -0.3836,  -2.5413,  -1.5520,\n",
       "           0.9593,   0.9490,   2.5349,   3.4287,  -2.5208,  -3.3501,  -1.4585,\n",
       "          -6.6375,  -8.1450,  -2.5488,  -0.8129,   1.5925,  -1.0689,  -1.5069,\n",
       "           3.3821,   3.6107,   4.5306,  -4.1186,   0.9992,  -1.4753,  -1.9164,\n",
       "           1.9871,  -5.9008,   0.3173,  -0.1720],\n",
       "        [ -2.4532,  -3.0258,   5.2253,  -2.8147,   0.4041,  -7.1005,  -9.2512,\n",
       "          -4.0248,  -3.6919,  -0.1330,  -0.6812,  -0.5600,   0.2983,  -0.2980,\n",
       "           2.1269,  -4.6184,   3.8216,   0.6124,   3.6335,   1.1796,  -1.2590,\n",
       "         -11.9950, -14.7049,  -5.4569, -14.1237,   1.0820, -12.3658,  -2.9393,\n",
       "           4.1095,  11.7725,  -7.9211,  -8.7318],\n",
       "        [ -1.3734,   0.0388,   0.5073,  -1.2695,  -0.3597,  -1.4701,  -0.5408,\n",
       "          -1.4293,  -0.4984,   0.7661,   1.6293,   1.5113,   0.8249,  -0.3904,\n",
       "           2.2003,  -0.5381,   2.0508,  -2.3375,   1.1881,   1.4566,  -1.7635,\n",
       "          -1.5520,  -0.4522,  -4.0956,  -2.1946,  -1.8836,  -1.4922,  -3.6931,\n",
       "          -0.3004,   2.5477,   0.7936,  -0.6701],\n",
       "        [ -1.3701,  -1.9959,   4.4220,  -1.5177,   2.7985,  -4.0968,  -2.5345,\n",
       "           1.3348,   2.0451,  -1.3323,   0.4239,  -3.7115,  -1.0412,  -1.0052,\n",
       "          -3.6359,  -8.3813,  -2.1490,   2.0555,   1.9745,  -4.3170,   3.5991,\n",
       "          -0.2394,  -6.1454,   6.5088,  -9.1088,   4.3074,  -6.2546,   1.8317,\n",
       "           1.9798,  -0.9761,  -3.7100,  -4.6492]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_linear.enable_adapter()\n",
    "lora_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ada7966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -3.3009,  -2.4167,   3.1948,  -5.2583,  -0.3836,  -2.5413,  -1.5520,\n",
       "           0.9593,   0.9490,   2.5349,   3.4287,  -2.5208,  -3.3501,  -1.4585,\n",
       "          -6.6375,  -8.1450,  -2.5488,  -0.8129,   1.5925,  -1.0689,  -1.5069,\n",
       "           3.3821,   3.6107,   4.5306,  -4.1186,   0.9992,  -1.4753,  -1.9164,\n",
       "           1.9871,  -5.9008,   0.3173,  -0.1720],\n",
       "        [ -2.4532,  -3.0258,   5.2253,  -2.8147,   0.4041,  -7.1005,  -9.2512,\n",
       "          -4.0248,  -3.6919,  -0.1330,  -0.6812,  -0.5600,   0.2983,  -0.2980,\n",
       "           2.1269,  -4.6184,   3.8216,   0.6124,   3.6335,   1.1796,  -1.2590,\n",
       "         -11.9950, -14.7049,  -5.4569, -14.1237,   1.0820, -12.3658,  -2.9393,\n",
       "           4.1095,  11.7725,  -7.9211,  -8.7318],\n",
       "        [ -1.3734,   0.0388,   0.5073,  -1.2695,  -0.3597,  -1.4701,  -0.5408,\n",
       "          -1.4293,  -0.4984,   0.7661,   1.6293,   1.5113,   0.8249,  -0.3904,\n",
       "           2.2003,  -0.5381,   2.0508,  -2.3375,   1.1881,   1.4566,  -1.7635,\n",
       "          -1.5520,  -0.4522,  -4.0956,  -2.1946,  -1.8836,  -1.4922,  -3.6931,\n",
       "          -0.3004,   2.5477,   0.7936,  -0.6701],\n",
       "        [ -1.3701,  -1.9959,   4.4220,  -1.5177,   2.7985,  -4.0968,  -2.5345,\n",
       "           1.3348,   2.0451,  -1.3323,   0.4239,  -3.7115,  -1.0412,  -1.0052,\n",
       "          -3.6359,  -8.3813,  -2.1490,   2.0555,   1.9745,  -4.3170,   3.5991,\n",
       "          -0.2394,  -6.1454,   6.5088,  -9.1088,   4.3074,  -6.2546,   1.8317,\n",
       "           1.9798,  -0.9761,  -3.7100,  -4.6492]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_linear = lora_linear.get_merged_module()\n",
    "merged_linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60a18b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3112, -0.5434,  0.5286, -0.3006, -0.2723, -0.3482, -0.1714, -0.2795,\n",
       "         -0.8456,  0.3614,  0.3751, -0.4710, -0.4060, -0.4528,  0.1198, -0.3354,\n",
       "         -0.0631, -0.3033,  0.2364,  0.2626, -0.4088, -0.4054, -0.3306,  0.0158,\n",
       "         -0.2222,  0.3794, -0.4798, -0.3367, -0.1746, -0.5843,  0.0875,  0.4903],\n",
       "        [-0.2201, -0.4519,  0.5687,  0.7261, -1.4542, -0.3455, -0.5674, -0.5083,\n",
       "          0.5267,  0.2598, -0.2588, -0.1397,  0.1017,  0.8528,  0.0521,  0.9468,\n",
       "         -0.0486, -0.1825, -0.4143, -0.6748, -0.2408,  0.4069,  0.4898,  0.9341,\n",
       "         -0.2967,  0.3735, -0.4919, -0.0398,  1.0945,  0.5666, -0.0376,  0.5202],\n",
       "        [-0.5604,  0.6451,  0.1797, -0.6332,  0.6717,  0.2009,  1.1105, -1.0070,\n",
       "          0.3122, -0.3347,  0.2356,  0.0648,  0.9382, -0.4550,  0.6496, -0.6390,\n",
       "         -0.1045, -0.5934,  0.7739, -0.2603,  0.4633,  0.9035,  0.7268, -0.6977,\n",
       "         -0.5872, -0.4794,  0.0224, -0.4526,  0.2953,  0.4776,  0.3914, -0.1528],\n",
       "        [-0.3303, -0.8498,  0.5489,  1.1919,  0.7467, -0.3974,  0.9824, -0.1678,\n",
       "          0.1932,  0.6756,  0.6278, -0.0841,  0.8882, -0.8669, -0.0345, -0.6671,\n",
       "          0.5580, -0.5825,  0.7146, -0.9386, -0.1250,  0.4492, -0.2384,  0.1074,\n",
       "         -0.2847, -0.1802, -0.2668,  0.1220,  1.0887,  0.4816,  0.8641,  0.3498]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_linear.disable_adapter()\n",
    "lora_linear(x)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5149c9f2-cf1b-4b22-a01f-4b8db55a1b7c",
   "metadata": {},
   "source": [
    "lora_linear.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8598ada-6479-4d31-8662-b664801b26a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
